[
  {
    "path": "posts/media-data/",
    "title": "Media Data",
    "description": "DACSS 603 Final Project Work: \"Media Analysis\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": {}
      }
    ],
    "date": "2022-05-04",
    "categories": [
      "statistics",
      "media analysis",
      "quantitative data analysis",
      "final project",
      "IT Army of Ukraine"
    ],
    "contents": "\r\n\r\nContents\r\nReading in Data\r\nPublic Interest\r\nMedia Interest\r\nMedia, Population, and\r\nUser Data\r\nProportional Data\r\nBasic Pearson\r\nCorrelation\r\n\r\n\r\nLinear Regression\r\nSimple\r\nCorrelation of Population and Sample Population\r\nMultiple Linear\r\nCorrelation of Variables\r\nInteraction Term\r\nSingle Models\r\nMultiple Lines\r\nStargazer\r\n\r\n\r\n\r\nReading in Data\r\nRead in data for both measures of media interest gathered from the\r\nSpike/Newswhip media site\r\nPublic Interest = Social media interactions on articles\r\nMedia Interest = Number of articles published\r\nPublic Interest\r\n\r\n\r\nShow code\r\n\r\n#public interest\r\npublic_interest <- read_csv(\"IT_Army_ Public_Interest.csv\", \r\n    col_types = cols(Date = col_date(format = \"%m/%d/%Y\")))\r\npublic_interest <- as_tibble(public_interest)\r\nhead(public_interest)\r\n\r\n\r\n# A tibble: 6 x 10\r\n  Date       All_Locations Europe North_America Oceania South_America\r\n  <date>             <dbl>  <dbl>         <dbl>   <dbl>         <dbl>\r\n1 2022-02-26         19811   7777          4763    2301             0\r\n2 2022-02-27        410149 346425         56690    3703           171\r\n3 2022-02-28        295464 238816         53122     278           166\r\n4 2022-03-01        202270 171920         28329     662            34\r\n5 2022-03-02         95478  73628         21099     836            12\r\n6 2022-03-03         33566   8929         24598     164             6\r\n# ... with 4 more variables: Asia <dbl>, Africa <dbl>,\r\n#   Middle_East <dbl>, Southeast_Asia <dbl>\r\n\r\nMedia Interest\r\n\r\n\r\nShow code\r\n\r\n#media interest\r\nmedia_interest <- read_csv(\"IT_Army_ Media_Interest.csv\", \r\n    col_types = cols(Date = col_date(format = \"%m/%d/%Y\")))\r\nmedia_interest <- as_tibble(media_interest)\r\nhead(media_interest)\r\n\r\n\r\n# A tibble: 6 x 10\r\n  Date       All_Locations Europe North_America Oceania South_America\r\n  <date>             <dbl>  <dbl>         <dbl>   <dbl>         <dbl>\r\n1 2022-02-26           134     15            61      16             0\r\n2 2022-02-27            89     14            42       4             2\r\n3 2022-02-28           356     46           249       4             1\r\n4 2022-03-01           106     29            46       8             0\r\n5 2022-03-02           152     26            75       3             0\r\n6 2022-03-03           109     11            62       1             0\r\n# ... with 4 more variables: Asia <dbl>, Africa <dbl>,\r\n#   Middle_East <dbl>, Southeast_Asia <dbl>\r\n\r\nMedia, Population, and User\r\nData\r\nAll measure are in one dataset for analysis.\r\n\r\n\r\nShow code\r\n\r\n#data frame with all regional observations\r\nregional_all <- read_csv(\"comprehensive_by_region.csv\") \r\nregional_all <- as_tibble(regional_all)\r\nregional_all\r\n\r\n\r\n# A tibble: 8 x 6\r\n  Region  Total_Population Sample_Populati~ DDOS_Users Public_Interest\r\n  <chr>              <dbl>            <dbl>      <dbl>           <dbl>\r\n1 Africa        1234685606        459973244        164            7666\r\n2 Asia          4434971532       3314507290       1443           29078\r\n3 Europe         748481333        745852328      40262          914764\r\n4 Middle~        393498810        303663005        390            1511\r\n5 North_~        600504974        512581441       5245          291484\r\n6 Oceania         43693399         30801415        718            9700\r\n7 South_~        437360279        404401418        393             735\r\n8 Southe~        680855171        614900482        264            9033\r\n# ... with 1 more variable: Media_Interest <dbl>\r\n\r\nProportional Data\r\n\r\n\r\nShow code\r\n\r\n#proportions\r\nproportions <- read.csv(\"proportions.csv\")\r\nproportions\r\n\r\n\r\n          Region Total_Population Sample_Population   DDOS_Users\r\n1         Africa      0.082521173       0.030742670 1.096110e-08\r\n2           Asia      0.296414773       0.221527673 9.644400e-08\r\n3         Europe      0.050025332       0.049849621 2.690942e-06\r\n4    Middle_East      0.026299799       0.020295553 2.606600e-08\r\n5  North_America      0.040135217       0.034258779 3.505537e-07\r\n6        Oceania      0.002920282       0.002058637 4.798810e-08\r\n7  South_America      0.029231315       0.027028483 2.626650e-08\r\n8 Southeast_Asia      0.045505485       0.041097352 1.764460e-08\r\n  Public_Interest Media_Interest\r\n1    5.123631e-07    1.33670e-09\r\n2    1.943451e-06    2.05186e-08\r\n3    6.113896e-05    1.53054e-08\r\n4    1.009889e-07    2.20560e-09\r\n5    1.948156e-05    6.02190e-08\r\n6    6.483070e-07    4.01010e-09\r\n7    4.912430e-08    3.34200e-10\r\n8    6.037276e-07    6.54990e-09\r\n\r\nBasic Pearson Correlation\r\n\r\n\r\nShow code\r\n\r\ncor(regional_all$DDOS_Users, regional_all$Public_Interest, method = \"pearson\")\r\n\r\n\r\n[1] 0.9811875\r\n\r\nLinear Regression\r\nSimple\r\nCorrelation of Population and Sample Population\r\nComparing the population of each region being examined and the\r\nrepresentative population of the countries represented in my sample.\r\nThese are highly correlated.\r\n\r\n\r\nShow code\r\n\r\n#Linear regression\r\npop_lm <- lm(Total_Population ~ Sample_Population, data = regional_all, na.action = na.exclude)\r\nsummary(pop_lm)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = Total_Population ~ Sample_Population, data = regional_all, \r\n    na.action = na.exclude)\r\n\r\nResiduals:\r\n       Min         1Q     Median         3Q        Max \r\n-253750322 -121385069  -57833324    -973754  611162518 \r\n\r\nCoefficients:\r\n                   Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)       1.419e+07  1.298e+08   0.109    0.917    \r\nSample_Population 1.325e+00  1.032e-01  12.837 1.37e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 283600000 on 6 degrees of freedom\r\nMultiple R-squared:  0.9649,    Adjusted R-squared:  0.959 \r\nF-statistic: 164.8 on 1 and 6 DF,  p-value: 1.373e-05\r\n\r\nMultiple Linear\r\nCorrelation of Variables\r\nExplanatory variables are both the public interest and media interest\r\ncounts with the DDOS user counts as the outcome variable.\r\n\r\n\r\nShow code\r\n\r\n#Linear regression user and media variables\r\nregional_mlm <- lm(DDOS_Users ~ Public_Interest + Media_Interest, data = regional_all, na.action = na.exclude)\r\nsummary(regional_mlm)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = DDOS_Users ~ Public_Interest + Media_Interest, data = regional_all, \r\n    na.action = na.exclude)\r\n\r\nResiduals:\r\n        1         2         3         4         5         6         7 \r\n-819.0092 2051.3093  167.6466 -199.0906 -704.4965   -0.2989 -410.6496 \r\n        8 \r\n -85.4111 \r\n\r\nCoefficients:\r\n                  Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)     815.022433 478.700459   1.703  0.14938    \r\nPublic_Interest   0.045171   0.001319  34.234    4e-07 ***\r\nMedia_Interest   -8.914703   1.411800  -6.314  0.00147 ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1060 on 5 degrees of freedom\r\nMultiple R-squared:  0.9958,    Adjusted R-squared:  0.9942 \r\nF-statistic: 599.5 on 2 and 5 DF,  p-value: 1.112e-06\r\n\r\n\r\n\r\nShow code\r\n\r\nlibrary(stargazer)\r\nstargazer(regional_mlm, type = \"text\")\r\n\r\n\r\n\r\n===============================================\r\n                        Dependent variable:    \r\n                    ---------------------------\r\n                            DDOS_Users         \r\n-----------------------------------------------\r\nPublic_Interest              0.045***          \r\n                              (0.001)          \r\n                                               \r\nMedia_Interest               -8.915***         \r\n                              (1.412)          \r\n                                               \r\nConstant                      815.022          \r\n                             (478.700)         \r\n                                               \r\n-----------------------------------------------\r\nObservations                     8             \r\nR2                             0.996           \r\nAdjusted R2                    0.994           \r\nResidual Std. Error     1,060.060 (df = 5)     \r\nF Statistic           599.468*** (df = 2; 5)   \r\n===============================================\r\nNote:               *p<0.1; **p<0.05; ***p<0.01\r\n\r\n\r\n\r\nShow code\r\n\r\n#create plot\r\nregional_plot <- ggplot(regional_all, aes(x = log(Media_Interest), y = log(DDOS_Users), color = Region)) +\r\n   geom_point() +\r\n   geom_smooth(method=lm,se=TRUE,fullrange=TRUE,color=\"cornflowerblue\") +\r\n   labs(title= \"Media Interest and Users\",\r\n        x= \"Media Interest\",\r\n        y = \"Users\") +\r\n    theme_minimal_hgrid()\r\nregional_plot\r\n\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n#create plot\r\nregional_plot2 <- ggplot(regional_all, aes(x = log(Public_Interest), y = log(DDOS_Users), color = Region)) +\r\n   geom_point() +\r\n   geom_smooth(method=lm,se=TRUE,fullrange=TRUE,color=\"cornflowerblue\") +\r\n   labs(title= \"Public Interest and Users\",\r\n        x= \"Public Interest\",\r\n        y = \"Users\") +\r\n    theme_minimal_hgrid()\r\nregional_plot2\r\n\r\n\r\n\r\n\r\nInteraction Term\r\nFitting another model, this time with an interaction term allowing\r\ninteraction between population and media interest\r\n\r\n\r\nShow code\r\n\r\nmlm3d <- lm(DDOS_Users ~ Public_Interest + Media_Interest + Public_Interest*Media_Interest, data = regional_all)\r\n\r\nsummary(mlm3d)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = DDOS_Users ~ Public_Interest + Media_Interest + \r\n    Public_Interest * Media_Interest, data = regional_all)\r\n\r\nResiduals:\r\n       1        2        3        4        5        6        7 \r\n-342.124   81.899    2.874  202.827   -4.138  111.255  252.096 \r\n       8 \r\n-304.690 \r\n\r\nCoefficients:\r\n                                 Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)                     1.008e+02  1.583e+02   0.637  0.55879\r\nPublic_Interest                 5.302e-02  1.050e-03  50.513 9.19e-07\r\nMedia_Interest                  2.492e-01  1.214e+00   0.205  0.84739\r\nPublic_Interest:Media_Interest -4.010e-05  5.038e-06  -7.959  0.00135\r\n                                  \r\n(Intercept)                       \r\nPublic_Interest                ***\r\nMedia_Interest                    \r\nPublic_Interest:Media_Interest ** \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 288.8 on 4 degrees of freedom\r\nMultiple R-squared:  0.9998,    Adjusted R-squared:  0.9996 \r\nF-statistic:  5405 on 3 and 4 DF,  p-value: 1.141e-07\r\n\r\nSingle Models\r\nPublic Interest\r\n\r\n\r\nShow code\r\n\r\npi <- lm(DDOS_Users ~ Public_Interest, data = regional_all)\r\nsummary(pi)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = DDOS_Users ~ Public_Interest, data = regional_all)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-6530.1   465.6   853.1   930.3  2035.0 \r\n\r\nCoefficients:\r\n                  Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)     -5.955e+02  1.158e+03  -0.514    0.625    \r\nPublic_Interest  4.244e-02  3.409e-03  12.449 1.64e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2899 on 6 degrees of freedom\r\nMultiple R-squared:  0.9627,    Adjusted R-squared:  0.9565 \r\nF-statistic:   155 on 1 and 6 DF,  p-value: 1.641e-05\r\n\r\nMedia Interest\r\n\r\n\r\nShow code\r\n\r\nmi <- lm(DDOS_Users ~ Media_Interest, data = regional_all)\r\nsummary(mi)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = DDOS_Users ~ Media_Interest, data = regional_all)\r\n\r\nResiduals:\r\n   Min     1Q Median     3Q    Max \r\n -5676  -5161  -4585  -4362  33997 \r\n\r\nCoefficients:\r\n               Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)    4678.357   6515.562   0.718    0.500\r\nMedia_Interest    6.928     18.681   0.371    0.723\r\n\r\nResidual standard error: 14850 on 6 degrees of freedom\r\nMultiple R-squared:  0.02241,   Adjusted R-squared:  -0.1405 \r\nF-statistic: 0.1375 on 1 and 6 DF,  p-value: 0.7235\r\n\r\nMultiple Lines\r\n\r\n\r\nShow code\r\n\r\ngg2 <- ggplot() +\r\n   geom_smooth(aes(log(DDOS_Users), log(Media_Interest)), data = regional_all, fullrange=TRUE,\r\n               method = \"lm\", se = TRUE, color = \"red3\") + \r\n   geom_smooth(aes(log(DDOS_Users), log(Public_Interest)), data = regional_all, fullrange=TRUE,\r\n               method = \"lm\", se = TRUE, color = \"forestgreen\") +\r\n  ggtitle(\"Media Interest and Public Interest & DDOS Users\", subtitle = \"Green Indicating Public Interest, Red Indicating Media Interest\") +\r\n  xlab(\"log: DDOS Users\") +\r\n  ylab(\"log: Media Interactions\") +\r\n  theme_minimal_hgrid()\r\n\r\n gg2 \r\n\r\n\r\n\r\n\r\nStargazer\r\nFitting the media models with stargazer\r\n\r\n\r\nShow code\r\n\r\nlibrary(stargazer)\r\nlibrary(tinytex)\r\nlibrary(sandwich)\r\n\r\nm1 = lm(DDOS_Users ~ Media_Interest, data = regional_all)\r\nm2 = lm(DDOS_Users ~ Public_Interest, data = regional_all)\r\n\r\nmodel.lst = list(m1, m2)\r\n\r\nstargazer(m1,\r\n          m2,\r\n          title=\"Displaying results for multiple response variables\",\r\n          type = \"text\",\r\n          float = TRUE,\r\n          report = \"vcsp\",\r\n          se=lapply(model.lst, function(x) sqrt(diag(vcovHC(x, type = \"HC1\")))),\r\n          no.space = TRUE,\r\n          header=FALSE,\r\n          single.row = TRUE,\r\n          font.size = \"small\",\r\n          intercept.bottom = F,\r\n          covariate.labels = c(\"Intercept\", \"Media Interest\", \"Public Interest\"),\r\n          column.labels = c(\"Media Interest\", \"Public Interest\"),\r\n          column.separate = c(1, 1),\r\n          digits = 3,\r\n          t.auto = T,\r\n          p.auto = T\r\n          )\r\n\r\n\r\n\r\nDisplaying results for multiple response variables\r\n=====================================================================\r\n                                       Dependent variable:           \r\n                             ----------------------------------------\r\n                                            DDOS_Users               \r\n                                Media Interest      Public Interest  \r\n                                      (1)                 (2)        \r\n---------------------------------------------------------------------\r\nIntercept                    4,678.357 (5,155.242) -595.471 (807.308)\r\n                                   p = 0.365           p = 0.461     \r\nMedia Interest                   6.928 (8.020)                       \r\n                                   p = 0.388                         \r\nPublic Interest                                      0.042 (0.003)   \r\n                                                       p = 0.000     \r\n---------------------------------------------------------------------\r\nObservations                           8                   8         \r\nR2                                   0.022               0.963       \r\nAdjusted R2                         -0.141               0.957       \r\nResidual Std. Error (df = 6)      14,846.860           2,898.960     \r\nF Statistic (df = 1; 6)              0.138             154.983***    \r\n=====================================================================\r\nNote:                                     *p<0.1; **p<0.05; ***p<0.01\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/media-data/public_interest.png",
    "last_modified": "2022-05-13T01:47:31-05:00",
    "input_file": "media-data.knit.md",
    "preview_width": 700,
    "preview_height": 432
  },
  {
    "path": "posts/further-analysis/",
    "title": "Further Analysis",
    "description": "DACSS 603 Final Project Work: \"Further Analysis\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": {}
      }
    ],
    "date": "2022-04-26",
    "categories": [
      "statistics",
      "quantitative data analysis",
      "final project",
      "IT Army of Ukraine"
    ],
    "contents": "\r\n\r\nContents\r\nReading in Data\r\nMultiple Linear Regression\r\nImportance to\r\nRespondents\r\nPolitical\r\nInclinations of Respondents\r\nDemographics of\r\nRespondents\r\nUsers to Politics and\r\nEducation\r\n\r\nSimple Linear Regression\r\nPotential\r\nCorrelation with Strike Propensity\r\nPotential\r\nCorrelation with Education Level\r\n\r\nAdding Population Weights\r\nImportance to\r\nRespondents\r\nPolitical\r\nInclinations of Respondents\r\nDemographics of\r\nRespondents\r\n\r\nWorking Data\r\n\r\nReading in Data\r\nContinuing this post by reading in the data I put together in my\r\nexploratory analysis. A full accounting of the variables and\r\ndescriptions are in the “About” tab of this GitHub Page.\r\n\r\n\r\nShow code\r\n\r\n#all IVS data\r\nivs_clean <- read.csv(\"ivs-df-clean.csv\")\r\nivs_clean <- as_tibble(ivs_clean)\r\nnames(ivs_clean)[1] <- 'country'\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n#integrated data\r\nall_data <- read.csv(\"integrated_data.csv\")\r\nall_data <- as_tibble(all_data)\r\nnames(all_data)[1] <- 'country'\r\nhead(all_data)\r\n\r\n\r\n# A tibble: 6 x 23\r\n  country   population region    users family friends leisure politics\r\n  <chr>     <chr>      <chr>     <int>  <dbl>   <dbl>   <dbl>    <dbl>\r\n1 Albania   3,088,385  Southern~    57   1.02    1.73    2.01     3.30\r\n2 Andorra   85,645     Southern~     6   1.12    1.54    1.42     2.94\r\n3 Argentina 45,864,941 South Am~    11   1.09    1.54    1.81     2.81\r\n4 Armenia   3,011,609  Western ~    16   1.11    1.74    1.99     2.79\r\n5 Australia 25,809,973 Oceania     717   1.11    1.48    1.65     2.41\r\n6 Austria   8,884,864  Western ~  3276   1.20    1.45    1.63     2.51\r\n# ... with 15 more variables: work <dbl>, religion <dbl>,\r\n#   willingness <dbl>, petition <dbl>, boycott <dbl>,\r\n#   demonstration <dbl>, strikes <dbl>, identity <dbl>,\r\n#   marital <dbl>, parents <dbl>, children <dbl>, household <dbl>,\r\n#   education <dbl>, income <dbl>, weights <dbl>\r\n\r\nMultiple Linear Regression\r\nImportance to Respondents\r\n\r\n\r\nShow code\r\n\r\n#Linear regression of \"importance\" variables\r\nmlm1 <- lm(users ~ family + friends + leisure + politics + work + religion, data = all_data, na.action = na.exclude)\r\nsummary(mlm1)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = users ~ family + friends + leisure + politics + \r\n    work + religion, data = all_data, na.action = na.exclude)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1678.1  -610.4  -342.4   130.6 11437.5 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)   -405.6     5035.6  -0.081    0.936\r\nfamily        3368.6     4464.5   0.755    0.453\r\nfriends       -730.0     1476.1  -0.495    0.623\r\nleisure        153.5     1244.6   0.123    0.902\r\npolitics     -1405.8      877.5  -1.602    0.114\r\nwork          1052.6     1416.3   0.743    0.460\r\nreligion       168.8      512.1   0.330    0.743\r\n\r\nResidual standard error: 1847 on 60 degrees of freedom\r\nMultiple R-squared:  0.1275,    Adjusted R-squared:  0.04025 \r\nF-statistic: 1.461 on 6 and 60 DF,  p-value: 0.2069\r\n\r\nPolitical Inclinations of\r\nRespondents\r\n\r\n\r\nShow code\r\n\r\n#Linear regression of \"politics\" variables\r\nmlm2 <- lm(users ~ willingness + petition + boycott + demonstration + strikes + identity, data = all_data, na.action = na.exclude)\r\nsummary(mlm2)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = users ~ willingness + petition + boycott + demonstration + \r\n    strikes + identity, data = all_data, na.action = na.exclude)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-2609.2  -623.3  -264.6   -26.5 10359.4 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)  \r\n(Intercept)    -212.45    5053.29  -0.042    0.967  \r\nwillingness    -687.64    1753.88  -0.392    0.697  \r\npetition      -1328.59    1274.62  -1.042    0.302  \r\nboycott        1454.15    1765.01   0.824    0.414  \r\ndemonstration -3373.22    1984.10  -1.700    0.095 .\r\nstrikes        3377.05    1452.75   2.325    0.024 *\r\nidentity         31.85     588.76   0.054    0.957  \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1893 on 53 degrees of freedom\r\n  (7 observations deleted due to missingness)\r\nMultiple R-squared:  0.1779,    Adjusted R-squared:  0.08481 \r\nF-statistic: 1.911 on 6 and 53 DF,  p-value: 0.09602\r\n\r\nDemographics of Respondents\r\n\r\n\r\nShow code\r\n\r\n#Linear regression of \"demographics\" variables\r\nmlm3 <- lm(users ~ marital + parents + children + household + education + income, data = all_data, na.action = na.exclude)\r\nsummary(mlm3)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = users ~ marital + parents + children + household + \r\n    education + income, data = all_data, na.action = na.exclude)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1678.0  -748.7  -399.7   136.5 11849.2 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)   3333.9     4885.7   0.682    0.498\r\nmarital       -437.5      692.4  -0.632    0.530\r\nparents      -2125.9     2979.5  -0.714    0.478\r\nchildren      -182.7      905.7  -0.202    0.841\r\nhousehold     -125.1      798.5  -0.157    0.876\r\neducation      534.3      405.4   1.318    0.193\r\nincome        -111.3      482.0  -0.231    0.818\r\n\r\nResidual standard error: 1863 on 59 degrees of freedom\r\n  (1 observation deleted due to missingness)\r\nMultiple R-squared:  0.1261,    Adjusted R-squared:  0.03726 \r\nF-statistic: 1.419 on 6 and 59 DF,  p-value: 0.2226\r\n\r\nUsers to Politics and\r\nEducation\r\n\r\n\r\nShow code\r\n\r\n#Linear regression of \"demographics\" variables\r\nmlm4 <- lm(users ~  politics + identity + education, data = all_data, na.action = na.exclude)\r\nsummary(mlm4)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = users ~ politics + identity + education, data = all_data, \r\n    na.action = na.exclude)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1829.8  -712.7  -307.7   154.6 11057.8 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)  \r\n(Intercept)   4948.5     5066.1   0.977   0.3329  \r\npolitics     -1943.4      923.4  -2.105   0.0398 *\r\nidentity      -509.0      607.1  -0.838   0.4054  \r\neducation      766.1      345.6   2.217   0.0307 *\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1866 on 56 degrees of freedom\r\n  (7 observations deleted due to missingness)\r\nMultiple R-squared:  0.1561,    Adjusted R-squared:  0.1109 \r\nF-statistic: 3.452 on 3 and 56 DF,  p-value: 0.02244\r\n\r\nSimple Linear Regression\r\nPotential\r\nCorrelation with Strike Propensity\r\nI am going to look further at the potential correlation between\r\ncountries with a propensity to engage in strikes and engage in DDOS\r\nattacks.\r\n\r\n\r\nShow code\r\n\r\n#Linear regression of \"politics\" variable \"strikes\"\r\nlm_strikes <- lm(users ~ strikes, data = all_data, na.action = na.exclude)\r\nsummary(lm_strikes)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = users ~ strikes, data = all_data, na.action = na.exclude)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n -827.2  -676.0  -592.6  -182.7 12475.0 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)   -222.8     2731.8  -0.082    0.935\r\nstrikes        355.3     1048.2   0.339    0.736\r\n\r\nResidual standard error: 1898 on 65 degrees of freedom\r\nMultiple R-squared:  0.001764,  Adjusted R-squared:  -0.01359 \r\nF-statistic: 0.1149 on 1 and 65 DF,  p-value: 0.7358\r\n\r\nPotential\r\nCorrelation with Education Level\r\nI am going to look further at the potential correlation between\r\neducation level with a propensity to engage in strikes and engage in\r\nDDOS attacks.\r\n\r\n\r\nShow code\r\n\r\n#Linear regression of \"educational level\" and \"users\"\r\nlm_education <- lm(users ~ education, data = all_data, na.action = na.exclude)\r\nsummary(lm_education)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = users ~ education, data = all_data, na.action = na.exclude)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1533.6  -810.1  -400.2   154.9 12164.3 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)  \r\n(Intercept)  -2730.8     1422.6  -1.920   0.0593 .\r\neducation      735.6      301.3   2.441   0.0174 *\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 1818 on 65 degrees of freedom\r\nMultiple R-squared:  0.084, Adjusted R-squared:  0.06991 \r\nF-statistic: 5.961 on 1 and 65 DF,  p-value: 0.01736\r\n\r\nIt may be that there are no correlations to be found here. But I want\r\nto run the analyses again using a column found in the codebook\r\nindicating ‘weights’ for population that should be used when comparing\r\nmultiple variables to account for population size.\r\nAdding Population Weights\r\nImportance to Respondents\r\nI’ll re-run the model with users and importance variables, but using\r\nthe weights column. This gives me a warning that this is an “essentially\r\nperfect fit” and that the summary may be reliable. This result is\r\nconsistent for each of the weighted models.\r\n\r\n\r\nShow code\r\n\r\n#Linear regression of \"importance\" variables + weighted   \r\nmlm1w <- lm(users ~ family + friends + leisure + politics + work + religion, data = all_data, na.action = na.exclude, weights)\r\nsummary(mlm1w)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = users ~ family + friends + leisure + politics + \r\n    work + religion, data = all_data, subset = weights, na.action = na.exclude)\r\n\r\nResiduals:\r\n         1        1.1 \r\n-5.024e-15  5.024e-15 \r\n\r\nCoefficients: (6 not defined because of singularities)\r\n             Estimate Std. Error   t value Pr(>|t|)    \r\n(Intercept) 5.700e+01  5.024e-15 1.134e+16   <2e-16 ***\r\nfamily             NA         NA        NA       NA    \r\nfriends            NA         NA        NA       NA    \r\nleisure            NA         NA        NA       NA    \r\npolitics           NA         NA        NA       NA    \r\nwork               NA         NA        NA       NA    \r\nreligion           NA         NA        NA       NA    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 7.105e-15 on 1 degrees of freedom\r\n\r\nPolitical Inclinations\r\nof Respondents\r\n\r\n\r\nShow code\r\n\r\n#Linear regression of \"politics\" variables + weighted\r\nmlm2w <- lm(users ~ willingness + petition + boycott + demonstration + strikes + identity, data = all_data, na.action = na.exclude, weights)\r\nsummary(mlm2w)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = users ~ willingness + petition + boycott + demonstration + \r\n    strikes + identity, data = all_data, subset = weights, na.action = na.exclude)\r\n\r\nResiduals:\r\n         1        1.1 \r\n-5.024e-15  5.024e-15 \r\n\r\nCoefficients: (6 not defined because of singularities)\r\n               Estimate Std. Error   t value Pr(>|t|)    \r\n(Intercept)   5.700e+01  5.024e-15 1.134e+16   <2e-16 ***\r\nwillingness          NA         NA        NA       NA    \r\npetition             NA         NA        NA       NA    \r\nboycott              NA         NA        NA       NA    \r\ndemonstration        NA         NA        NA       NA    \r\nstrikes              NA         NA        NA       NA    \r\nidentity             NA         NA        NA       NA    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 7.105e-15 on 1 degrees of freedom\r\n\r\nDemographics of Respondents\r\n\r\n\r\nShow code\r\n\r\n#Linear regression of \"demographics\" variables + weighted\r\nmlm3w <- lm(users ~ marital + parents + children + household + education + income, data = all_data, na.action = na.exclude, weights)\r\nsummary(mlm3w)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = users ~ marital + parents + children + household + \r\n    education + income, data = all_data, subset = weights, na.action = na.exclude)\r\n\r\nResiduals:\r\n         1        1.1 \r\n-5.024e-15  5.024e-15 \r\n\r\nCoefficients: (6 not defined because of singularities)\r\n             Estimate Std. Error   t value Pr(>|t|)    \r\n(Intercept) 5.700e+01  5.024e-15 1.134e+16   <2e-16 ***\r\nmarital            NA         NA        NA       NA    \r\nparents            NA         NA        NA       NA    \r\nchildren           NA         NA        NA       NA    \r\nhousehold          NA         NA        NA       NA    \r\neducation          NA         NA        NA       NA    \r\nincome             NA         NA        NA       NA    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 7.105e-15 on 1 degrees of freedom\r\n\r\nWorking Data\r\nThis time I will start with the data frames with the observations I\r\nam looking at but not summarized by mean:\r\n\r\n\r\nShow code\r\n\r\nall_data <- read.csv(\"integrated_data.csv\")\r\nnames(all_data)[1] <- 'country'\r\nhead(all_data)\r\n\r\n\r\n    country population          region users   family  friends\r\n1   Albania  3,088,385 Southern Europe    57 1.020308 1.734266\r\n2   Andorra     85,645 Southern Europe     6 1.115538 1.535857\r\n3 Argentina 45,864,941   South America    11 1.091725 1.541376\r\n4   Armenia  3,011,609    Western Asia    16 1.110073 1.738318\r\n5 Australia 25,809,973         Oceania   717 1.110372 1.479615\r\n6   Austria  8,884,864  Western Europe  3276 1.197929 1.454047\r\n   leisure politics     work religion willingness petition  boycott\r\n1 2.011888 3.295518 1.198740 2.154654    1.794562 2.245353 2.598047\r\n2 1.421315 2.940120 1.525948 2.965070    1.448551 1.645000 2.574277\r\n3 1.809810 2.807193 1.468405 2.393939    1.611392 2.098532 2.732899\r\n4 1.986604 2.789367 1.468625 1.828494    1.870714 2.296271 2.593623\r\n5 1.646355 2.411832 1.995392 2.897727    1.553846 1.218576 2.078859\r\n6 1.632765 2.513431 1.670122 2.641498    1.471685 1.599877 2.355781\r\n  demonstration  strikes identity  marital  parents children\r\n1      2.477688 2.959712 3.761194 2.004888 1.562544 2.172943\r\n2      2.054835 2.349650 4.748322 2.844311 1.108000 1.211155\r\n3      2.237895 2.234528 5.617571 3.225324 1.244267 1.406147\r\n4      2.314614 2.585122 4.534380 2.818971 1.594793 1.790387\r\n5      2.016750 2.193404 5.068860 2.521739 1.068409 1.786682\r\n6      2.303878 2.709257 5.074932 2.980428 1.100487 1.368615\r\n  household education   income   weights\r\n1  4.087352  4.019512 4.089326 0.6968641\r\n2  2.500996  4.856431 5.302405 0.9960159\r\n3  3.500502  3.735793 5.073452 0.9970090\r\n4  4.262339  5.140000 4.008415 0.6666667\r\n5  2.480541  5.655192 5.085401 0.5515720\r\n6  2.287897  4.528624 4.468501 0.6082725\r\n\r\nRather than just using a linear model, I want to look at some basic\r\nvisualizations of relationships. I’ll start by just looking at which\r\ncountries have the highest mean values assigned by respondents to the\r\nquestion of how important politics is to them.\r\n\r\n\r\nShow code\r\n\r\nimp_plot <- arrange(all_data, desc(politics))\r\nhead(imp_plot)\r\n\r\n\r\n                 country population          region users   family\r\n1                Albania  3,088,385 Southern Europe    57 1.020308\r\n2               Slovenia  2,102,106 Southern Europe    18 1.154350\r\n3 Bosnia and Herzegovina  3,824,782 Southern Europe    11 1.077191\r\n4                Romania 21,230,362  Eastern Europe   558 1.100453\r\n5                 Serbia  6,974,289 Southern Europe    95 1.092483\r\n6                Croatia  4,208,973 Southern Europe    34 1.170715\r\n   friends  leisure politics     work religion willingness petition\r\n1 1.734266 2.011888 3.295518 1.198740 2.154654    1.794562 2.245353\r\n2 1.617537 1.707635 3.124883 1.458101 2.788785    1.800617 1.933206\r\n3 1.572838 1.683324 3.111370 1.444186 1.840816    1.508497 1.866467\r\n4 1.843117 1.705800 3.094499 1.572877 1.759441    1.574477 2.383695\r\n5 1.453795 1.612470 3.007564 1.608609 2.150040    1.538356 1.972892\r\n6 1.629130 1.754410 3.002051 1.609973 2.355281    1.688716 1.546648\r\n   boycott demonstration  strikes identity  marital  parents children\r\n1 2.598047      2.477688 2.959712 3.761194 2.004888 1.562544 2.172943\r\n2 2.357418      2.291587 2.570605 4.620647 2.629699 1.300562 1.473340\r\n3 2.300725      2.261733 2.507051 4.880546 2.957746 1.294256 1.401063\r\n4 2.792772      2.456101 2.750283 4.903103 2.533929 1.269908 1.412312\r\n5 2.304253      2.274639 2.393001 4.902655 3.026232 1.387430 1.230677\r\n6 2.198257      2.175840 2.370640 4.635749 3.054953 1.266938 1.366892\r\n  household education   income   weights\r\n1  4.087352  4.019512 4.089326 0.6968641\r\n2  2.973758  4.693171 4.942987 0.9302326\r\n3  2.928404  4.259977 4.489199 0.5800464\r\n4  2.833214  4.240461 4.526850 0.6968641\r\n5  3.149361  5.257552 4.870781 0.7337250\r\n6  2.903491  4.858204 5.572056 0.6724950\r\n\r\nNext, I’ll use the “broom” package to look at the linear relationship\r\nbetween all variables and save it in a dataframe:\r\n\r\n\r\nShow code\r\n\r\ndata_lm <- lm(users~family + friends + leisure + politics + work + religion, data = all_data, na.action = na.exclude)\r\ntidylm_important <- tidy(data_lm)\r\ntidylm_important\r\n\r\n\r\n# A tibble: 7 x 5\r\n  term        estimate std.error statistic p.value\r\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\r\n1 (Intercept)    -406.     5036.   -0.0805   0.936\r\n2 family         3369.     4465.    0.755    0.453\r\n3 friends        -730.     1476.   -0.495    0.623\r\n4 leisure         154.     1245.    0.123    0.902\r\n5 politics      -1406.      877.   -1.60     0.114\r\n6 work           1053.     1416.    0.743    0.460\r\n7 religion        169.      512.    0.330    0.743\r\n\r\n\r\n\r\nShow code\r\n\r\ndata_lm2 <- lm(users~willingness + petition + boycott + demonstration + strikes + identity, data = all_data, na.action = na.exclude)\r\ntidylm_politics <- tidy(data_lm2)\r\ntidylm_politics\r\n\r\n\r\n# A tibble: 7 x 5\r\n  term          estimate std.error statistic p.value\r\n  <chr>            <dbl>     <dbl>     <dbl>   <dbl>\r\n1 (Intercept)     -212.      5053.   -0.0420  0.967 \r\n2 willingness     -688.      1754.   -0.392   0.697 \r\n3 petition       -1329.      1275.   -1.04    0.302 \r\n4 boycott         1454.      1765.    0.824   0.414 \r\n5 demonstration  -3373.      1984.   -1.70    0.0950\r\n6 strikes         3377.      1453.    2.32    0.0240\r\n7 identity          31.9      589.    0.0541  0.957 \r\n\r\n\r\n\r\nShow code\r\n\r\ndata_lm3 <- lm(users~marital + parents + children + household + education + income, data = all_data, na.action = na.exclude)\r\ntidylm_demographics <- tidy(data_lm3)\r\ntidylm_demographics\r\n\r\n\r\n# A tibble: 7 x 5\r\n  term        estimate std.error statistic p.value\r\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\r\n1 (Intercept)    3334.     4886.     0.682   0.498\r\n2 marital        -438.      692.    -0.632   0.530\r\n3 parents       -2126.     2980.    -0.714   0.478\r\n4 children       -183.      906.    -0.202   0.841\r\n5 household      -125.      798.    -0.157   0.876\r\n6 education       534.      405.     1.32    0.193\r\n7 income         -111.      482.    -0.231   0.818\r\n\r\n\r\n\r\nShow code\r\n\r\nall_data %>% \r\n  ggplot(aes(log(users), log(education))) +\r\n  geom_point() +\r\n  geom_smooth(method=lm,se=TRUE,fullrange=TRUE,color=\"goldenrod\") +\r\n    theme_minimal_hgrid()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/further-analysis/education.png",
    "last_modified": "2022-05-12T23:04:07-05:00",
    "input_file": "further-analysis.knit.md",
    "preview_width": 700,
    "preview_height": 432
  },
  {
    "path": "posts/exploratory-analysis/",
    "title": "Exploratory Analysis",
    "description": "DACSS 603 Final Project Work: \"Analyzing Data\"",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": {}
      }
    ],
    "date": "2022-04-24",
    "categories": [
      "statistics",
      "quantitative data analysis",
      "final project",
      "IT Army of Ukraine"
    ],
    "contents": "\r\n\r\nContents\r\nData Sources\r\nDDOS User Observations\r\nWVS/EVS\r\nSpike/Newswhip\r\n\r\nData Analysis\r\nDDOS Users\r\nDDOS Daily Observations\r\nDDOS Cumulative\r\nObservations\r\nDDOS Regional\r\nObservations\r\nPopulation & User Data\r\nOnly\r\nLinear Model of\r\nPopulation and Users\r\n\r\nIVS Data\r\nReading in Data\r\nTransforming IVS Data\r\n\r\nMatching Data\r\nUsing Scaled Data\r\n\r\n\r\n\r\nData Sources\r\nDDOS User Observations\r\nThe primary data is a set of observations of users of a novice\r\n“hacking tool” to engage in DDOS (denial of service) attacks against\r\nRussian targets in March 2022. The data contains a total of users\r\ncumulatively for each day of the series March 2 through March 11, and\r\nthe users represent participants from 98 counties.\r\nWVS/EVS\r\nI will also be using a data set of observations from the World Values\r\nSurvey conducted from 2017-2021 as a joint project between the World\r\nValues Survey and the European Values Studies. This data was released in\r\nJuly 2021, and contains responses from ~135,000 respondents among 95\r\ncountries.\r\nSpike/Newswhip\r\nThe third is a data set of media coverage (media articles and social\r\nmedia mentions) of the Ukrainian minister’s call for volunteers for the\r\n“IT Army of Ukraine” to help fight the invasion of Russia on the digital\r\nfront.\r\nData Analysis\r\nDDOS Users\r\nI moved the data into various forms to best explore ways to analyze\r\nit.\r\nDDOS Daily Observations\r\nCountry Name\r\nPopulation (as indicated by the U.S. CIA World factbook\r\nwebsite)\r\nRegion (as indicated by the U.S. CIA World factbook website)\r\nColumns for each date being observed from March 2 - March 11 of DDOS\r\nusers from each country. This is difficult to use for analysis because\r\nthe daily observations do NOT represent new users added on that day;\r\nrather, the daily observations represent the cumulative users from each\r\ncountry as of that day.\r\n\r\n\r\nShow code\r\n\r\n#load the data\r\nddos_daily <- read_csv(\"ddos_observations.csv\")\r\n#assign column names to represent variables accurately\r\ncolnames(ddos_daily) <- c(\"Country\", \"Population\", \"Region\", \"March2\", \"March3\", \"March4\", \"March5\", \"March6\", \"March7\", \"March8\", \"March9\", \"March10\", \"March11\")\r\n#summarize the data\r\noptions(scipen = 999)\r\nhead(ddos_daily)\r\n\r\n\r\n# A tibble: 6 x 13\r\n  Country  Population Region March2 March3 March4 March5 March6 March7\r\n  <chr>         <dbl> <chr>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\r\n1 Aland         29789 Europe      1      1      1      1      1      1\r\n2 Albania     3088385 Europe     19     22     22     23     32     44\r\n3 Algeria    43576691 Africa      0      8      8      8      8      8\r\n4 Andorra       85645 Europe      2      6      6      6      6      6\r\n5 Argenti~   45864941 South~      9      9      9     11     11     11\r\n6 Armenia     3011609 Asia        1      7      7      9      9     13\r\n# ... with 4 more variables: March8 <dbl>, March9 <dbl>,\r\n#   March10 <dbl>, March11 <dbl>\r\n\r\nThe total DDOS users as of the first day of observations, March 2,\r\n2022, and the last day available for observation, March 11, 2022 began\r\nat 7,850 and grew to a total of 48,879.\r\n\r\n\r\nShow code\r\n\r\nsum(ddos_daily$March2)\r\n\r\n\r\n[1] 7850\r\n\r\nShow code\r\n\r\nsum(ddos_daily$March11)\r\n\r\n\r\n[1] 48879\r\n\r\nDDOS Cumulative Observations\r\nHowever, I am not going to examine the panel data; I am only going to\r\nlook at the cumulative data - or the count of users on the last day of\r\nobservations, March 11. So this looks at:\r\nCountry Name\r\nPopulation (as indicated by the U.S. CIA World factbook\r\nwebsite)\r\nRegion (as indicated by the UN classifications)\r\nUsers of the DDOS tool from each country as of the last day\r\nobserved, March 11\r\n\r\n\r\nShow code\r\n\r\n#load the data\r\nddos_cumulative <- read_csv(\"ddos_cumulative.csv\")\r\n#summarize the data\r\noptions(scipen = 999)\r\nhead(ddos_cumulative)\r\n\r\n\r\n# A tibble: 6 x 4\r\n  country   population region        users\r\n  <chr>          <dbl> <chr>         <dbl>\r\n1 Aland          29789 Europe            1\r\n2 Albania      3088385 Europe           57\r\n3 Algeria     43576691 Africa           10\r\n4 Andorra        85645 Europe            6\r\n5 Argentina   45864941 South America    11\r\n6 Armenia      3011609 Asia             16\r\n\r\nDDOS Regional Observations\r\nIt is still important to be able to visualize the dramatic change in\r\nuser count over time, even if I am not analyzing the time series in this\r\nanalysis. I experimented with displaying the increase as a whole and the\r\nincrease by region. So this looks at:\r\nDate of observations\r\nUsers of the DDOS tool from each region as of the the given day\r\n\r\n\r\nShow code\r\n\r\nddos_regions <- read_csv(\"ddos_by_region.csv\", \r\n    col_types = cols(Date = col_date(format = \"%m/%d/%Y\")))\r\nddos_regions <- as_tibble(ddos_regions) \r\nddos_regions\r\n\r\n\r\n# A tibble: 10 x 10\r\n   Date       Africa  Asia Europe Middle_East North_America Oceania\r\n   <date>      <dbl> <dbl>  <dbl>       <dbl>         <dbl>   <dbl>\r\n 1 2022-03-02     15   180   4863          72          1208      90\r\n 2 2022-03-03     32   419   6994         115          1723     119\r\n 3 2022-03-04     39   467   9069         137          1905     135\r\n 4 2022-03-05     59   604  17392         163          2416     177\r\n 5 2022-03-06     77   694  18447         184          2653     195\r\n 6 2022-03-07     88   867  20999         206          3057     245\r\n 7 2022-03-08    129  1143  27081         306          4028     363\r\n 8 2022-03-09    137  1171  27996         320          4245     580\r\n 9 2022-03-10    156  1308  30141         353          4548     623\r\n10 2022-03-11    164  1443  34439         390          5245     718\r\n# ... with 3 more variables: South_America <dbl>,\r\n#   Southeast_Asia <dbl>, Ukraine <dbl>\r\n\r\n\r\n\r\nShow code\r\n\r\nggplot(ddos_regions, aes(x = Date)) +\r\n  geom_line(aes(y = Africa, colour = \"Africa\")) +\r\n  geom_line(aes(y = Asia, colour = \"Asia\")) +\r\n  geom_line(aes(y = Europe, colour = \"Europe\")) +\r\n  geom_line(aes(y = Middle_East, colour = \"Middle East\")) +\r\n  geom_line(aes(y = North_America, colour = \"North America\")) +\r\n  geom_line(aes(y = Oceania, colour = \"Oceania\")) +\r\n  geom_line(aes(y = South_America, colour = \"South America\")) +\r\n  geom_line(aes(y = Southeast_Asia, colour = \"Southeast Asia\")) + \r\n  geom_line(aes(y = Ukraine, colour = \"Ukraine\")) +\r\n  scale_colour_discrete((name = \"Region\")) +\r\n  xlab(\"Dates\") +\r\n  ylab(\"Users\") +\r\n  ggtitle(\"Increase in Regional Users by Date\") +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nIf we eliminate the most significant location of users (Europe) it is\r\nsimply easier to get an idea of how the users from the remaining regions\r\nincreased over time.\r\n\r\n\r\nShow code\r\n\r\nggplot(ddos_regions, aes(x = Date)) +\r\n  geom_line(aes(y = Africa, colour = \"Africa\")) +\r\n  geom_line(aes(y = Asia, colour = \"Asia\")) +\r\n  geom_line(aes(y = Middle_East, colour = \"Middle East\")) +\r\n  geom_line(aes(y = North_America, colour = \"North America\")) +\r\n  geom_line(aes(y = Oceania, colour = \"Oceania\")) +\r\n  geom_line(aes(y = South_America, colour = \"South America\")) +\r\n  geom_line(aes(y = Southeast_Asia, colour = \"Southeast Asia\")) + \r\n  geom_line(aes(y = Ukraine, colour = \"Ukraine\")) +\r\n  scale_colour_discrete((name = \"Region\")) +\r\n  xlab(\"Dates\") +\r\n  ylab(\"Users\") +\r\n  ggtitle(\"Increase in Non-European Users by Date\") +\r\n  theme_minimal()\r\n\r\n\r\n\r\n\r\nAnd the total users over time.\r\n\r\n\r\nShow code\r\n\r\nddos_time <- read_csv(\"daily_observations.csv\", \r\n    col_types = cols(Date = col_date(format = \"%m/%d/%Y\")))\r\nddos_time <- as_tibble(ddos_time) \r\ngg <- ggplot(ddos_time, aes(x = Date)) +\r\n  geom_line(aes(y = Total)) +\r\n  xlab(\"Dates\") +\r\n  ylab(\"Users\") +\r\n  ggtitle(\"Increase in Total Users by Date\") +\r\n  theme_minimal()\r\ngg\r\n\r\n\r\n\r\n\r\nPopulation & User Data Only\r\nI’ll start with a basic visualization of the relationship between the\r\npopulation of the countries and the number of users of DDOS attacks from\r\nthe corresponding countries:\r\n\r\n\r\nShow code\r\n\r\n#create plot\r\nggplot(ddos_cumulative, aes(x = log(population), y = log(users), color = region)) +\r\n  geom_point () +\r\n  facet_wrap(\"region\")\r\n\r\n\r\n\r\n\r\nLinear Model of Population\r\nand Users\r\nWhat I want to look at is the linear model of the relationship\r\nbetween the population of each country with participating users and the\r\ncorresponding sample of users from that country.\r\nI’ll first simplify my data set to only contain the columns I am\r\nlooking at here.\r\n\r\n\r\nShow code\r\n\r\npop_users <- ddos_cumulative %>% \r\n  select(c(population, users))\r\ngg1 <- ggplot(pop_users, aes(x=population, y=users)) +\r\n   geom_point() +\r\n   geom_smooth(method=lm,se=TRUE,fullrange=TRUE,color=\"cornflowerblue\") +\r\n   labs(title= \"Population and Users\",\r\n        x= \"Population\",\r\n        y = \"Users\") +\r\n    theme_minimal_hgrid()\r\ngg1\r\n\r\n\r\n\r\n\r\nThat’s a mess. I want to take the log() of the data to achieve a\r\nbetter look at the model\r\n\r\n\r\ngg1b <- ggplot(pop_users, aes(x=log(population), y=log(users))) +\r\n  geom_point() +\r\n  geom_smooth(method=lm,se=TRUE,fullrange=TRUE,color=\"cornflowerblue\") +\r\n   labs(title= \"Log: Population and Users\",\r\n        x= \"Population (log)\",\r\n        y = \"Users (log)\") +\r\n   theme_minimal_hgrid()\r\n\r\ngg1b\r\n\r\n\r\n\r\n\r\nOn first look at this relationship, it seems clear that there is no\r\ncorrelation between a country’s population and the number of users of\r\nthe DDOS tool.\r\n\r\n\r\nShow code\r\n\r\npop_users_lm <- lm(population~users, data = pop_users)\r\nsummary(pop_users_lm)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = population ~ users, data = pop_users)\r\n\r\nResiduals:\r\n       Min         1Q     Median         3Q        Max \r\n -68122950  -59609854  -52417541  -18720617 1334465479 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)   \r\n(Intercept) 63129462   21142912   2.986  0.00359 **\r\nusers           4092      12789   0.320  0.74972   \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 199600000 on 96 degrees of freedom\r\nMultiple R-squared:  0.001065,  Adjusted R-squared:  -0.009341 \r\nF-statistic: 0.1024 on 1 and 96 DF,  p-value: 0.7497\r\n\r\nIVS Data\r\nThe next data source I want to explore is the IVS data set.\r\nReading in Data\r\nThis brings in an overwhelming 135,000 observations of 231 variables.\r\nI selected the columns I am interested in working with and saved as a\r\n.csv file, which I will read in for the rest of the analysis.\r\nA full accounting of the variables and descriptions are in the\r\n“About” tab of this GitHub Page.\r\nTo make matching easier, I used the “countrycode” package to assign\r\nproper country names to the ISO-3 numeric code from the data set.\r\n\r\n\r\nShow code\r\n\r\n#read in .dta file\r\n#library(haven)\r\n#ivs_data <- read_dta(\"data/ivs/ZA7505_v2-0-0.dta\")\r\n#head(ivs_data[33:42])\r\n#write.csv(ivs_data, file = \"./data/ivs/ivs_data.csv\", row.names = TRUE)\r\n#select relevant data\r\n#ivs_subset <- select(ivs_data,10,34,35,40:50,106,109:114,119:138,150:162,166,188:196,199,201,210:214,222,224,225,230,231)\r\n#ivs_df <- as.data.frame(ivs_subset)\r\n#load package for converting country codes\r\n#library(countrycode)\r\n#ivs_df$country.name <- countrycode(ivs_df$cntry, origin = \"iso3n\", destination = \"country.name\")\r\n\r\nivs_clean <- read.csv(\"ivs-df-clean.csv\")\r\nivs_clean <- as_tibble(ivs_clean)\r\nnames(ivs_clean)[1] <- 'country'\r\nhead(ivs_clean)\r\n\r\n\r\n# A tibble: 6 x 72\r\n  country weight imp_family imp_friends imp_leisure imp_politics\r\n  <chr>    <dbl>      <int>       <int>       <int>        <int>\r\n1 Albania  0.697          2           1           2            3\r\n2 Albania  0.697          1           1           4            4\r\n3 Albania  0.697          1           2           2            4\r\n4 Albania  0.697          1           2           2            4\r\n5 Albania  0.697          1           1           2            4\r\n6 Albania  0.697          1           3           3            4\r\n# ... with 66 more variables: imp_work <int>, imp_religion <int>,\r\n#   sat_happiness <int>, sat_health <int>, sat_life <int>,\r\n#   sat_control <int>, willingness_fight <int>,\r\n#   interest_politics <int>, prop_petition <int>,\r\n#   prop_boycotts <int>, prop_demonstrations <int>,\r\n#   prop_strikes <int>, self_position <int>, conf_churches <int>,\r\n#   conf_armed <int>, conf_press <int>, conf_unions <int>, ...\r\n\r\nTransforming IVS Data\r\nPreprocessing\r\nIn the original data in the IVS datasets, there are some meaningless\r\nchoices in the value labels such as “Not asked,” “NA,” and “DK.”\r\nAdditionally, some response have negative serial numbers. Furthermore, I\r\nexcluded variables that have a response structure that do not follow the\r\nstructures that are congruous to the structure of the majority of the\r\nresponses.\r\nGrouping by Mean\r\nThere are some changes needed to make the data more manageable. I\r\nhave cleaned up some of the data by assigning all negative values\r\nrepresenting various codes for no available observation to “NA” when\r\napplicable. I took means when applicable saved the resulting means by\r\ncountry as a series of data sets I saved offline that I will import.\r\nExample of\r\nhow I manipulated the data before saving:\r\n\r\n\r\nShow code\r\n\r\n#select relevant data\r\n#ivs_important <- select(ivs_clean,1:8)\r\n\r\n#find mean of each column\r\n#important <- ivs_important %>%\r\n  #group_by(country) %>%\r\n  #summarise(\r\n    #family = mean(imp_family, na.rm = TRUE),\r\n    #friends = mean(imp_friends, na.rm = TRUE),\r\n    #leisure = mean(imp_leisure, na.rm = TRUE),\r\n    #politics = mean(imp_politics, na.rm = TRUE),\r\n    #work = mean(imp_work, na.rm = TRUE),\r\n    #religion = mean(imp_religion, na.rm = TRUE)\r\n    #)\r\n\r\n\r\n\r\nLooking at\r\ndata frames representing country means:\r\n\r\n\r\nShow code\r\n\r\nimportant <- read_csv(\"important.csv\")\r\nhead(important)\r\n\r\n\r\n# A tibble: 6 x 8\r\n     X1 country   family friends leisure politics  work religion\r\n  <dbl> <chr>      <dbl>   <dbl>   <dbl>    <dbl> <dbl>    <dbl>\r\n1     1 Albania     1.02    1.73    2.01     3.30  1.20     2.15\r\n2     2 Andorra     1.12    1.54    1.42     2.94  1.53     2.97\r\n3     3 Argentina   1.09    1.54    1.81     2.81  1.47     2.39\r\n4     4 Armenia     1.11    1.74    1.99     2.79  1.47     1.83\r\n5     5 Australia   1.11    1.48    1.65     2.41  2.00     2.90\r\n6     6 Austria     1.20    1.45    1.63     2.51  1.67     2.64\r\n\r\nMatching Data\r\nWhen eliminating the countries who did not have a profile in the IVS\r\ndataset from my observation data, I lost approximately 2,000\r\nobservations and have 67 countries to compare. I created a data frame of\r\nthis information to use going forward.\r\n\r\n\r\nShow code\r\n\r\nall_data <- read_csv(\"integrated_data.csv\")\r\nhead(all_data)\r\n\r\n\r\n# A tibble: 6 x 23\r\n  country   population region    users family friends leisure politics\r\n  <chr>          <dbl> <chr>     <dbl>  <dbl>   <dbl>   <dbl>    <dbl>\r\n1 Albania      3088385 Southern~    57   1.02    1.73    2.01     3.30\r\n2 Andorra        85645 Southern~     6   1.12    1.54    1.42     2.94\r\n3 Argentina   45864941 South Am~    11   1.09    1.54    1.81     2.81\r\n4 Armenia      3011609 Western ~    16   1.11    1.74    1.99     2.79\r\n5 Australia   25809973 Oceania     717   1.11    1.48    1.65     2.41\r\n6 Austria      8884864 Western ~  3276   1.20    1.45    1.63     2.51\r\n# ... with 15 more variables: work <dbl>, religion <dbl>,\r\n#   willingness <dbl>, petition <dbl>, boycott <dbl>,\r\n#   demonstration <dbl>, strikes <dbl>, identity <dbl>,\r\n#   marital <dbl>, parents <dbl>, children <dbl>, household <dbl>,\r\n#   education <dbl>, income <dbl>, scaled_weights <dbl>\r\n\r\nUsing Scaled Data\r\nNormalization\r\nSome of the variables have different value labels and maximum values,\r\neven within the same family of topics. For example, I may want to\r\nnormalize the user scale when looking at, for example, the first set of\r\nvariables that have responses on a scale of 1 to 4 accordingly?\r\n\r\n\r\nShow code\r\n\r\nall_data <- read.csv(\"integrated_data.csv\")\r\nscale_4 <- rescale(all_data$users, to=c(1,4))\r\nsummary(scale_4)\r\n\r\n\r\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n  1.000   1.004   1.019   1.159   1.109   4.000 \r\n\r\nShow code\r\n\r\nscale_users_4 <- as.data.frame(scale_4)\r\nhead(scale_users_4)\r\n\r\n\r\n   scale_4\r\n1 1.012742\r\n2 1.001138\r\n3 1.002275\r\n4 1.003413\r\n5 1.162912\r\n6 1.745165\r\n\r\nLinear Regression: Scaled\r\nData\r\n\r\n\r\nShow code\r\n\r\n#Join scaled values of users to summary data\r\nall_data$scaled_users <- scale_4\r\n#Linear regression of \"importance\" variables + scaled user variable  \r\nlm_imp <- lm(scaled_users ~ family + friends + leisure + politics + work + religion, data = all_data, na.action = na.exclude)\r\nsummary(lm_imp)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = scaled_users ~ family + friends + leisure + politics + \r\n    work + religion, data = all_data, na.action = na.exclude)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.38183 -0.13888 -0.07790  0.02971  2.60239 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)  0.90748    1.14576   0.792    0.431\r\nfamily       0.76647    1.01582   0.755    0.453\r\nfriends     -0.16610    0.33585  -0.495    0.623\r\nleisure      0.03493    0.28318   0.123    0.902\r\npolitics    -0.31986    0.19966  -1.602    0.114\r\nwork         0.23951    0.32225   0.743    0.460\r\nreligion     0.03842    0.11651   0.330    0.743\r\n\r\nResidual standard error: 0.4202 on 60 degrees of freedom\r\nMultiple R-squared:  0.1275,    Adjusted R-squared:  0.04025 \r\nF-statistic: 1.461 on 6 and 60 DF,  p-value: 0.2069\r\n\r\nLinear Regression: Unscaled\r\nData\r\nCompare that to the un-scaled user data. I’m not sure that scaling\r\nwill make a difference in the data integrity using regression analysis\r\ngoing forward.\r\nHowever, this is very informative to me as a novice user of linear\r\nmodels to understand how scaling affects the degrees of freedom, but not\r\nthe adjusted R-squared or p-values.\r\n\r\n\r\nShow code\r\n\r\n#Linear regression of \"importance\" variables + unscaled user variable  \r\nlm_imp2 <- lm(users ~ family + friends + leisure + politics + work + religion, data = all_data, na.action = na.exclude)\r\nsummary(lm_imp2)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = users ~ family + friends + leisure + politics + \r\n    work + religion, data = all_data, na.action = na.exclude)\r\n\r\nResiduals:\r\n    Min      1Q  Median      3Q     Max \r\n-1678.1  -610.4  -342.4   130.6 11437.5 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)   -405.6     5035.6  -0.081    0.936\r\nfamily        3368.6     4464.5   0.755    0.453\r\nfriends       -730.0     1476.1  -0.495    0.623\r\nleisure        153.5     1244.6   0.123    0.902\r\npolitics     -1405.8      877.5  -1.602    0.114\r\nwork          1052.6     1416.3   0.743    0.460\r\nreligion       168.8      512.1   0.330    0.743\r\n\r\nResidual standard error: 1847 on 60 degrees of freedom\r\nMultiple R-squared:  0.1275,    Adjusted R-squared:  0.04025 \r\nF-statistic: 1.461 on 6 and 60 DF,  p-value: 0.2069\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/exploratory-analysis/usersbyday.png",
    "last_modified": "2022-05-12T18:02:18-05:00",
    "input_file": "exploratory-analysis.knit.md",
    "preview_width": 6300,
    "preview_height": 6300
  },
  {
    "path": "posts/welcome/",
    "title": "Project Info and Data Sources",
    "description": "Welcome to the project page for the quantitative analysis of data gathered concerning the call to create an 'IT Army of Ukraine'.",
    "author": [
      {
        "name": "Kristina Becvar",
        "url": {}
      }
    ],
    "date": "2022-04-20",
    "categories": [
      "statistics",
      "final project",
      "IT Army of Ukraine",
      "data sources"
    ],
    "contents": "\r\n\r\nContents\r\nIT Army Project\r\nPurpose\r\nBackground\r\nHypothesis\r\nData\r\nTelegraph IT Army\r\nChannel DDOS Usage\r\nIVS Data\r\n\r\nCitations\r\n\r\n\r\nIT Army Project\r\nPurpose\r\nThis is a small study of characteristics of users of pre-made DDOS\r\ntools in the wake of Ukraine’s call to create an ‘IT Army’ to combat\r\ndisinformation by engaging in ever-changing activities directed through\r\na Telegram channel created for the purpose.\r\nBackground\r\nWhen Russia invaded Ukraine in February 2021, Ukraine’s vice minister\r\nof technology called for those with the desire and skill to assist the\r\nfight against Russian disinformation. The method of action was to be\r\ncommunicated via a channel on the social media platform “Telegram”, and\r\nthe title of the group is “IT Army of Ukraine”. The call to join this\r\ncause was announced on Twitter, and was amplified by numerous regular\r\nand social media platforms.\r\nAs this happened during the period of my studies on social data\r\nanalysis, I joined the group to observe if there was anything to study\r\nunder this topic. After a couple of days, the leaders of the Telegram\r\nchannel put a welcome post directing those joining the channel to ways\r\nthey can contribute based on their knowledge of “hacking” or overall\r\nprogramming knowledge. The primary recommendation to those with no to\r\nminimal knowledge of hacking tools was to click on a link to a “DDOS\r\ntool” that the user could click on and leave open in their browser that\r\nwas pre-programmed to a set of Russian media sites that the channel\r\nleaders claimed were spreading misinformation about the war.\r\nOne thing to note is that upon clicking on the link, a ‘warning’ box\r\npopped up for the user that informed them that engaging in assisting in\r\nthis manner may not be legal in the user’s geographical location. If the\r\nuser clicks “accept” to that warning, the script begins running.\r\nWhen exiting the tool, the page GitHub repository link was listed on\r\nthe bottom of the site. In the corresponding GitHub repository, the\r\nperson who designed the page had added in a module to track the location\r\nof the users who had “accepted” the warning and continued on to use the\r\nDDOS tool.\r\nThis repository is where I gathered the data for this project, on a\r\ndaily basis, as of midnight GMT/UTC time.\r\nHypothesis\r\nI am looking at a model where the outcome is the number of DDOS\r\nattacks originated from a given country and explanatory variables are\r\nWVS activism scores, media coverage, and other controls.\r\nData\r\nTelegraph IT Army Channel\r\nDDOS Usage\r\nThe primary data is the set of observations of users of one novice\r\n“hacking tool” to engage in DDOS (denial of service) attacks against\r\nRussian targets in March 2022. The data contains a total of users\r\ncumulatively for each day of the series March 2 through March 11, and\r\nthe users represent participants from 98 counties.\r\nIVS Data\r\nAdditional data is from the IVS (Integrated Values Study); a\r\ncollaboration of the European Values Study and World Values Study.\r\nThe European Value Study (EVS) and the World Value Survey (WVS) are\r\ntwo large-scale, cross-national and repeated cross-sectional survey\r\nresearch programs. They include a large number of questions, which have\r\nbeen replicated since the early eighties.\r\nIn line with the Memorandum of Understandings, both organizations\r\nagreed to cooperate in joint data collection from 2017. EVS has been\r\nresponsible for planning and conducting surveys in European countries,\r\nusing the EVS questionnaire and EVS methodological guidelines. WVSA has\r\nbeen responsible for planning and conducting surveys in countries in the\r\nworld outside Europe, using the WVS questionnaire and WVS methodological\r\nguidelines. Five countries (Germany, Romania, Russia, Serbia, and\r\nUkraine) conducted surveys in both waves EVS 2017 and WVS7.\r\nBoth organizations developed their draft questionnaires\r\nindependently. The joint items define the Common Core of both\r\nquestionnaires. They are marked in yellow in the two Master\r\nQuestionnaires (EVS2017 MQ and WVS7 MQ).\r\nThe EVS archive (GESIS) has been responsible for data archiving and\r\ndata processing of the European surveys. The WVS archive (JDS Madrid)\r\nhas been responsible for data archiving and data processing for the\r\nnon-European surveys and the WVS surveys additionally conducted in\r\nGermany, Romania, Russia, Serbia, and Ukraine. Based on the Common\r\nEVS/WVS Dictionary agreed by EVS and WVS the Joint EVS/WVS 2017-2021\r\ndataset (Joint EVS/WVS) has been constructed in close collaboration.\r\nThe first version of the Joint EVS/WVS includes data and\r\ndocumentation of altogether 81 countries and territories: 35 from the\r\nEVS 2017, 51 from the WVS7. Five of these countries (Germany, Romania,\r\nRussia, Serbia, and Ukraine) conducted surveys in both waves EVS and\r\nWVS.\r\nCitations\r\nEVS (2021): EVS Trend File 1981-2017. GESIS Data Archive,\r\nCologne. ZA7503 Data file Version 2.0.0, doi:10.4232/1.13736.\r\nHaerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova,\r\nK., Diez-Medrano J., M. Lagos, P. Norris, E. Ponarin & B. Puranen et\r\nal. (eds.). 2021. World Values Survey Time-Series (1981-2020)\r\nCross-National Data-Set. Madrid, Spain & Vienna, Austria: JD Systems\r\nInstitute & WVSA Secretariat. Version 2.0.0, doi:10.14281/18241.15.\r\nAllison L, Wang C, Kaminsky J (2021) Religiosity, neutrality,\r\nfairness, skepticism, and societal tranquility: A data science analysis\r\nof the World Values Survey. PLoS ONE 16(1): e0245231. https://doi.org/10.1371/journal.pone.0245231\r\nLi, Q., Wang, B., Deng, H., & Yu, C. (2018). A quantitative\r\nanalysis of global environmental protection values based on the world\r\nvalues survey data from 1994 to 2014. Environmental Monitoring and\r\nAssessment, 190(10), 593. https://doi.org/10.1007/s10661-018-6949-z\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-12T21:44:34-05:00",
    "input_file": "welcome.knit.md"
  }
]
