---
title: "Exploratory Analysis"
description: |
  DACSS 603 Final Project Work: "Analyzing Data"
preview: usersbyday.png
categories:
  - statistics
  - final project
  - IT Army of Ukraine
author:
  - name: Kristina Becvar
date: 2022-04-24
output:
  distill::distill_article:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(plyr); library(dplyr)
library(ggplot2)
library(cowplot)
library(broom)
library(scales)
library(here)
```

# Data Sources

## DDOS User Observations

The primary data is a set of observations of users of a novice "hacking tool" to engage in DDOS (denial of service) attacks against Russian targets in March 2022. The data contains a total of users cumulatively for each day of the series March 2 through March 11, and the users represent participants from 98 counties.

## WVS/EVS

I will also be using a data set of observations from the World Values Survey conducted from 2017-2021 as a joint project between the World Values Survey and the European Values Studies. This data was released in July 2021, and contains responses from ~135,000 respondents among 95 countries.

## Spike/Newswhip

The third is a data set of media coverage (media articles and social media mentions) of the Ukrainian minister's call for volunteers for the "IT Army of Ukraine" to help fight the invasion of Russia on the digital front. 

# Data Analysis

## DDOS Users

I moved the data into various forms to best explore ways to analyze it.

### DDOS Daily Observations

* Country Name
* Population (as indicated by the U.S. CIA World factbook website)
* Region (as indicated by the UN classifications)
* Columns for each date being observed from March 2 - March 11 of DDOS users from each country. This is difficult to use for analysis because the daily observations do NOT represent new users added on that day; rather, the daily observations represent the culumative users from each country as of that day.

```{r code_folding = TRUE}
#load the data
ddos_daily <- read_csv("active_observations.csv")
#assign column names to represent variables accurately
colnames(ddos_daily) <- c("Country", "Population", "Region", "March2", "March3", "March4", "March5", "March6", "March7", "March8", "March9", "March10", "March11")
#summarize the data
options(scipen = 999)
head(ddos_daily)
```

The total DDOS users as of the first day of observations, March 2, 2022, and the last day available for observation, March 11, 2022 began at 7,850 and grew to a total of 48,879.

```{r code_folding = TRUE}
sum(ddos_daily$March2)
sum(ddos_daily$March11)
```

### DDOS Cumulative Observations

However, I am not going to examine the panel data; I am only going to look at the cumulative data - or the count of users on the last day of observations, March 11. So this looks at:

* Country Name
* Population (as indicated by the U.S. CIA World factbook website)
* Region (as indicated by the UN classifications)
* Users of the DDOS tool from each country as of the last day observed, March 11

```{r code_folding = TRUE}
#load the data
ddos_cumulative <- read_csv("cumulative_observations.csv")
#summarize the data
options(scipen = 999)
head(ddos_cumulative)
```

### DDOS Regional Observations

It is still important to be able to visualize the dramatic change in user count over time, even if I am not analyzing the time series in this analysis. I experimented with displaying the increase as a whole and the increase by region. So this looks at:

* Date of observations
* Users of the DDOS tool from each region as of the the given day

```{r code_folding = TRUE}
ddos_regions <- read_csv("regional_observations.csv", 
    col_types = cols(Date = col_date(format = "%m/%d/%Y")))
ddos_regions <- as_tibble(ddos_regions) 
ddos_regions
```

```{r code_folding = TRUE}

ggplot(ddos_regions, aes(x = Date)) +
  geom_line(aes(y = Africa, colour = "Africa")) +
  geom_line(aes(y = Central_America, colour = "Central America")) +
    geom_line(aes(y = Central_America, colour = "Central America")) +
    geom_line(aes(y = Central_Asia, colour = "Central Asia")) +
    geom_line(aes(y = Eastern_Europe, colour = "Eastern Europe")) +
    geom_line(aes(y = North_America, colour = "North America")) +
    geom_line(aes(y = Northern_Europe, colour = "Northern Europe")) +
    geom_line(aes(y = Oceania, colour = "Oceania")) +
    geom_line(aes(y = Southeastern_Asia, colour = "Southeastern Asia")) +
    geom_line(aes(y = South_America, colour = "South America")) +
    geom_line(aes(y = Southern_Europe, colour = "Southern Europe")) +
    geom_line(aes(y = Western_Asia, colour = "Western Asia")) +
    geom_line(aes(y = Western_Europe, colour = "Western Europe")) +
  scale_colour_discrete((name = "Region")) +
  xlab("Dates") +
  ylab("Users") +
  ggtitle("Increase in Users by Date") +
  theme_minimal()
```

```{r code_folding = TRUE}
ddos_time <- read_csv("daily_observations.csv", 
    col_types = cols(Date = col_date(format = "%m/%d/%Y")))
ddos_time <- as_tibble(ddos_time) 
gg <- ggplot(ddos_time, aes(x = Date)) +
  geom_line(aes(y = Total)) +
  xlab("Dates") +
  ylab("Users") +
  ggtitle("Increase in Users by Date") +
  theme_minimal()
gg
```

### Population & User Data Only

I'll start with a basic visualization of the relationship between the population of the countries and the number of users of DDOS attacks from the corresponding countries:

```{r code_folding=TRUE}
#create plot
ggplot(ddos_cumulative, aes(x = log(Population), y = log(Users), color = Region)) +
  geom_point () +
  facet_wrap("Region")
```

### Linear Model of Population and Users

What I want to look at is the linear model of the relationship between the population of each country with participating users and the corresponding sample of users from that country.

I'll first simplify my data set to only contain the columns I am looking at here.

```{r code_folding=TRUE}
pop_df <- ddos_cumulative %>% 
  select(c(Population, Users))
gg1 <- ggplot(pop_df, aes(x=Population, y=Users)) +
   geom_point() +
   geom_smooth(method=lm,se=TRUE,fullrange=TRUE,color="cornflowerblue") +
   labs(title= "Population and Users",
        x= "Population",
        y = "Users") +
    theme_minimal_hgrid()
gg1

```

That's a mess. I want to take the log() of the data to achieve a better look at the model

```{r}
gg1b <- ggplot(pop_df, aes(x=log(Population), y=log(Users))) +
  geom_point() +
  geom_smooth(method=lm,se=TRUE,fullrange=TRUE,color="cornflowerblue") +
   labs(title= "Log: Population and Users",
        x= "Population (log)",
        y = "Users (log)") +
   theme_minimal_hgrid()

gg1b
```

On first look at this relationship, it seems clear that there is no correlation between a country's population and the number of users of the DDOS tool.

```{r code_folding=TRUE}
ddos_pop_lm <- lm(Population~Users, data = pop_df)
summary(ddos_pop_lm)
```

## IVS Data

The next data source I want to explore is the IVS data set. 

### Reading in Data

This brings in an overwhelming 135,000 observations of 231 variables. I selected the columns I am interested in working with and saved as a .csv file, which I will read in for the rest of the analysis.

To make matching easier, I used the "countrycode" package to assign proper country names to the ISO-3 numeric code from the data set.

```{r code_folding = TRUE}
#read in .dta file
#library(haven)
#ivs_data <- read_dta("data/ivs/ZA7505_v2-0-0.dta")
#head(ivs_data[33:42])
#write.csv(ivs_data, file = "./data/ivs/ivs_data.csv", row.names = TRUE)
#select relevant data
#ivs_subset <- select(ivs_data,10,34,35,40:50,106,109:114,119:138,150:162,166,188:196,199,201,210:214,222,224,225,230,231)
#ivs_df <- as.data.frame(ivs_subset)
#load package for converting country codes
#library(countrycode)
#ivs_df$country.name <- countrycode(ivs_df$cntry, origin = "iso3n", destination = "country.name")

ivs_clean <- read.csv("ivs_df_names.csv")
ivs_clean <- as_tibble(ivs_clean)
names(ivs_clean)[1] <- 'country'
head(ivs_clean)
```

### Transforming IVS Data

#### Preprocessing

In the original data in the IVS datasets, there are some meaningless choices in the value labels such as “Not asked,” “NA,” and “DK.” Additionally, some response have negative serial numbers. Furthermore, I excluded variables that have a response structure that do not follow the structures that are congruous to the structure of the majority of the responses.

#### Normalization

Some of the variables have different value labels and maximum values, even within the same family of topics. For example, I may want to normalize the user scale when looking at, for example, the first set of variables that have responses on a scale of 1 to 4 accordingly?

```{r code_folding = TRUE}
all_data <- read.csv("integrated_data.csv")
scale_4 <- rescale(all_data$users, to=c(1,4))
summary(scale_4)
scale_users_4 <- as.data.frame(scale_4)
head(scale_users_4)
```

### Importance to Respondents

There are some changes needed to make the data more manageable. I want to clean up some of the data by assigning all negative values representing various codes for no available observation to "NA" when applicable. 

```{r code_folding = TRUE}
important_na <- read.csv("important_na.csv")
important_na <- as_tibble(important_na)
names(important_na)[1] <- 'country'

#find mean of each column
important <- important_na %>%
  group_by(country) %>%
  summarise(
    family = mean(family, na.rm = TRUE),
    friends = mean(friends, na.rm = TRUE),
    leisure = mean(leisure, na.rm = TRUE),
    politics = mean(politics, na.rm = TRUE),
    work = mean(work, na.rm = TRUE),
    religion = mean(religion, na.rm = TRUE)
  )
summary(important)

shapiro.test(all_data$users)

```

### Political Inclinations of Respondents

```{r code_folding = TRUE}
#read in the 
politics_na <- read_csv("politics_na.csv", 
    col_types = cols(willing_fight = col_double(), 
        interest_politics = col_double(), 
        sign_petition = col_double(), join_boycott = col_double(), 
        attend_demonstration = col_double(), 
        join_strikes = col_double(), self_id = col_double()))
names(politics_na)[1] <- 'country'
#find mean of each column
politics <- politics_na %>%
  group_by(country) %>%
  summarise(
    willingness = mean(willing_fight, na.rm = TRUE),
    petition = mean(sign_petition, na.rm = TRUE),
    boycott = mean(join_boycott, na.rm = TRUE),
    demonstration = mean(attend_demonstration, na.rm = TRUE),
    strikes = mean(join_strikes, na.rm = TRUE),
    identity = mean(self_id, na.rm = TRUE)
  )

summary(politics)
```

### Demographics of Respondents

```{r code_folding = TRUE}
#read in the data
demographics_na <- read_csv("demographics_na.csv",
    col_types = cols(marital_status = col_double(), 
        live_parents = col_double(), children = col_double(), 
        household = col_double(), highest_ed = col_double(), 
        employment = col_double(), income = col_double()))
#find mean of each column
demographics <- demographics_na %>%
  group_by(country) %>%
  summarise(
    marital = mean(marital_status, na.rm = TRUE),
    parents = mean(live_parents, na.rm = TRUE),
    children = mean(children, na.rm = TRUE),
    household = mean(household, na.rm = TRUE),
    education = mean(highest_ed, na.rm = TRUE),
    income = mean(income, na.rm = TRUE)
  )

summary(demographics)
```

## Matching Data

When eliminating the countries who did not have a profile in the IVS dataset from my observation data, I lost approximately 2,000 observations and have 67 countries to compare. I created a data frame of this information to use going forward.

```{r code_folding = TRUE}
all_data <- read_csv("integrated_data.csv")
head(all_data)
```


# Using Scaled Data

```{r code_folding = TRUE}
#Join scaled values of users to summary data
all_data$users4 <- scale_4
#Linear regression of "importance" variables + scaled user variable  
lm_imp <- lm(users4 ~ family + friends + leisure + politics + work + religion, data = all_data, na.action = na.exclude)
summary(lm_imp)
```

Compare that to the un-scaled user data. I'm not sure that scaling will make a difference in the data integrity using regression analysis going forward.

```{r code_folding = TRUE}

#Linear regression of "importance" variables + unscaled user variable  
lm_imp2 <- lm(users ~ family + friends + leisure + politics + work + religion, data = all_data, na.action = na.exclude)
summary(lm_imp2)
```
